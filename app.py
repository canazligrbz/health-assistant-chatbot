import nest_asyncio
nest_asyncio.apply()
import os
from dotenv import load_dotenv
import warnings

warnings.filterwarnings("ignore")
import streamlit as st

from rag_index_builder import load_existing_store, EMBEDDING_MODEL
from haystack import Pipeline
from haystack.components.embedders import SentenceTransformersTextEmbedder
from haystack_integrations.components.retrievers.qdrant import QdrantEmbeddingRetriever
from haystack.components.builders import ChatPromptBuilder
from haystack.dataclasses import ChatMessage
from haystack_integrations.components.generators.google_genai import GoogleGenAIChatGenerator
from haystack_integrations.components.common.google_genai.utils import Secret

# Ortam deÄŸiÅŸkenlerini yÃ¼kle
load_dotenv()
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
EMBEDDING_DIM = 384


# Rag pipeline kurulumu
@st.cache_resource
def initialize_rag_pipeline():
    """
    Qdrant DocumentStore'u yÃ¼kler ve RAG Sorgulama Pipeline'Ä±nÄ± kurar.
    Bu fonksiyon Streamlit cache'i sayesinde sadece bir kez Ã§alÄ±ÅŸÄ±r!
    """

    if not GOOGLE_API_KEY:
        st.error("HATA: GOOGLE_API_KEY yÃ¼klenemedi. LÃ¼tfen .env dosyanÄ±zÄ± kontrol edin.")
        return None

    # 1. Qdrant DocumentStore'u YÃ¼kle
    with st.spinner("Qdrant VeritabanÄ± hÄ±zlÄ±ca yÃ¼kleniyor..."):
        try:
            document_store = load_existing_store()

            if document_store is None:
                st.warning(
                    "VeritabanÄ± yÃ¼klenemedi. LÃ¼tfen 'rag_index_builder.py' dosyasÄ±nÄ± Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun.")
                return None

        except RuntimeError as e:
            if "already accessed" in str(e):
                st.error("Qdrant veritabanÄ± zaten baÅŸka bir sÃ¼reÃ§ tarafÄ±ndan kullanÄ±lÄ±yor.")
                st.info("Ã‡Ã¶zÃ¼m: TÃ¼m Python sÃ¼reÃ§lerini kapatÄ±p uygulamayÄ± yeniden baÅŸlatÄ±n.")
                st.stop()
            else:
                st.error(f"Beklenmeyen hata: {e}")
                return None

    # Sorgulama Pipeline'Ä±nÄ± kurma

    # Retriever, Qdrant'tan arama yapacak bileÅŸen
    retriever = QdrantEmbeddingRetriever(document_store=document_store, top_k=3)
    # Sorgu metnini vektÃ¶rlere Ã§evirecek bileÅŸen
    text_embedder = SentenceTransformersTextEmbedder(model=EMBEDDING_MODEL)

    try:
        gemini_secret = Secret.from_env_var("GOOGLE_API_KEY")
    except ValueError:
        st.error("HATA: GOOGLE_API_KEY ortam deÄŸiÅŸkeni ayarlanmadÄ± veya .env dosyasÄ± okunmadÄ±.")
        return None

    # Generator (LLM)
    generator = GoogleGenAIChatGenerator(model="gemini-2.0-flash-exp", api_key=gemini_secret)
    prompt_template = [
        ChatMessage.from_system(
            "Sen bir TÃ¼rkÃ§e saÄŸlÄ±k danÄ±ÅŸmanlÄ±ÄŸÄ± chatbotusun. "
            "RolÃ¼n, yalnÄ±zca saÄŸlanan doktor yanÄ±tlarÄ±nÄ± analiz ederek hastanÄ±n sorusuna uygun aÃ§Ä±klama Ã¼retmektir. "
            "Kendi tÄ±bbi yorumunu ekleme, varsayÄ±m yapma veya belgelerde yer almayan bilgileri kullanma. "
            "Her yanÄ±tÄ±n anlaÅŸÄ±lÄ±r, sakin ve profesyonel bir tonda olmalÄ±. "
            "EÄŸer belgelerde yeterli veya ilgili bilgi yoksa, bunu aÃ§Ä±kÃ§a belirt ('SaÄŸlanan bilgilere gÃ¶re bu konuda yeterli veri bulunmamaktadÄ±r. LÃ¼tfen doktorunuza danÄ±ÅŸÄ±nÄ±z.') "
            "BaÅŸka bir ÅŸey yazma. "
            "EÄŸer belgelerde yeterli bilgi varsa, yanÄ±tÄ±nÄ± yapÄ±landÄ±rÄ±lmÄ±ÅŸ biÃ§imde ve empatik bir kapanÄ±ÅŸla ver."
        ),
        ChatMessage.from_user(
            """Belgeler:
    {% for doc in documents %}
    - UzmanlÄ±k AlanÄ±: {{ doc.meta['doctor_speciality'] }}
    - Orijinal Soru: {{ doc.meta['question'] }}
    - Cevap Ä°Ã§eriÄŸi: {{ doc.content }}
    {% endfor %}

    Soru: {{question}}

    YanÄ±t formatÄ±:

    - EÄŸer belgelerde yeterli bilgi **yoksa**:
      "SaÄŸlanan bilgilere gÃ¶re bu konuda yeterli veri bulunmamaktadÄ±r. LÃ¼tfen doktorunuza danÄ±ÅŸÄ±nÄ±z." ifadesini **aynen** yaz. BaÅŸka hiÃ§bir ÅŸey ekleme.

    - EÄŸer belgelerde yeterli bilgi **varsa**:
      SaÄŸlanan bilgilere gÃ¶re:
      1. **KÄ±sa Ã–zet:** (sorunun genel yanÄ±tÄ±nÄ± 1-2 cÃ¼mlede aÃ§Ä±kla)
      2. **DetaylÄ± AÃ§Ä±klama:** (doktor cevaplarÄ±na dayalÄ± detaylarÄ± ve olasÄ± nedenleri aÃ§Ä±kla)
      3. **UyarÄ± veya Ã–neri:** (doktorlarÄ±n vurguladÄ±ÄŸÄ± Ã¶nemli noktalarÄ± belirt)
      4. **KaynakÃ§a:** 
         - YalnÄ±zca yanÄ±tla doÄŸrudan iliÅŸkili uzmanlÄ±k alanlarÄ±nÄ± listele.
      5. **Empatik KapanÄ±ÅŸ:** "GeÃ§miÅŸ olsun." ifadesini ekle.
    """
        )
    ]

    prompt_builder = ChatPromptBuilder(template=prompt_template)

    # RAG Sorgu Pipeline'Ä± OluÅŸturma ve BaÄŸlama
    rag_pipeline = Pipeline()
    rag_pipeline.add_component("text_embedder", text_embedder)
    rag_pipeline.add_component("retriever", retriever)
    rag_pipeline.add_component("prompt_builder", prompt_builder)
    rag_pipeline.add_component("generator", generator)

    rag_pipeline.connect("text_embedder.embedding", "retriever.query_embedding")
    rag_pipeline.connect("retriever.documents", "prompt_builder.documents")
    rag_pipeline.connect("prompt_builder.prompt", "generator.messages")

    st.sidebar.success("RAG Sistemi HazÄ±r!")
    return rag_pipeline


# Streamlit uygulamasÄ±:
def main():
    st.set_page_config(page_title="ğŸ©º TÃ¼rkÃ§e Hasta-Doktor Chatbotu", page_icon="ğŸ‡¹ğŸ‡·", layout="wide")

    # Sidebar: Proje Bilgileri ve Status
    st.sidebar.title("ğŸ“Š Proje Durumu")
    st.sidebar.markdown(f"**VeritabanÄ±:** Qdrant (KalÄ±cÄ±)")
    st.sidebar.markdown(f"**VektÃ¶r Boyutu:** 768")
    st.sidebar.markdown(f"**Belge SayÄ±sÄ±:** 332.036")

    # Ana BaÅŸlÄ±k
    st.markdown("## TÃ¼rkÃ§e Hasta-Doktor SaÄŸlÄ±k AsistanÄ±")
    st.caption(
        "Bu chatbot, doktorlarÄ±n verdiÄŸi kanÄ±tlara dayalÄ± olarak yanÄ±t Ã¼retir. LÃ¼tfen tÄ±bbi tavsiye almadÄ±ÄŸÄ±nÄ±zÄ± unutmayÄ±n.")

    # RAG Pipeline'Ä± YÃ¼kle/OluÅŸtur
    rag_pipeline = initialize_rag_pipeline()

    if rag_pipeline is None:
        return

    # Sohbet geÃ§miÅŸi (Streamlit session state)
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # GeÃ§miÅŸ mesajlarÄ± gÃ¶ster
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # KullanÄ±cÄ±dan girdi al
    if prompt := st.chat_input("Ã–rn: BaÅŸ aÄŸrÄ±sÄ± iÃ§in ne zaman doktora gitmeliyim?"):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # RAG boru hattÄ±nÄ± Ã§alÄ±ÅŸtÄ±r ve yanÄ±t al
        with st.chat_message("assistant"):
            with st.spinner("Ä°lgili doktor cevaplarÄ± taranÄ±yor ve yanÄ±t oluÅŸturuluyor..."):
                try:
                    result = rag_pipeline.run({
                        "text_embedder": {"text": prompt},
                        "prompt_builder": {"question": prompt}
                    })

                    response = result["generator"]["replies"][0].text
                    st.markdown(response)

                except Exception as e:
                    st.error(f"Sorgu iÅŸlenirken bir hata oluÅŸtu: {str(e)}")
                    response = "Sorgu hatasÄ±."

        st.session_state.messages.append({"role": "assistant", "content": response})


if __name__ == "__main__":
    main()
